{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Neural Network\n",
    "PyTorch provides all of the most common parts that are required to cobble together a neural network. In this notebook we will take a look at each piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "The easiest way to build our model is to create a class which inherits PyTorch's `nn.Module` class. An `nn.Module` stores layer weights and has a method `.forward(input)` that will be called when doing the forward pass through the network. Modules provide a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc. For this tutorial we will make a simple CNN for the MNIST dataset.  \n",
    "\n",
    "The `__init__` function is were we declare the model weights. These could be plain torch tensors, but PyTorch also provides pre-built version of common layers, such as linear, convolutional, or recurrent layers.  \n",
    "\n",
    "The `nn.Sequential` container class allows us to string together multiple operations that will be executed sequentially.\n",
    "\n",
    "In the `.forward(input)` function we define how the model input will interact with the model weights. Although we don't use it in this example, we can use normal python control flow here to change how the model reacts to certain scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (conv1): Sequential (\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (conv2): Sequential (\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (fc): Linear (3136 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a CNN model object\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=5,\n",
    "                      padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, \n",
    "                      out_channels=64, \n",
    "                      kernel_size=5,\n",
    "                      padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "\n",
    "        # PyTorch can't infer the number of inputs to layers (yet), \n",
    "        # so we need to manually calculate the number of outputs from the convolutional layers\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 10)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # At least here we can use the .view function to infer the output size \n",
    "        # in order to flatten the convolutional layer output\n",
    "        out = out.view(out.size(0), -1)  \n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # There is no need to add a softmax in the forward pass as it is included in the loss function\n",
    "        return out\n",
    "    \n",
    "cnn = CNN()  # Instantiate the model\n",
    "print(cnn)   # Printing the model will show us each of the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. The PyTorch `nn` package provides a variety of different loss functions. We will use `nn.CrossEntropyLoss` which combines `LogSoftMax` and `NLLLoss` into a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Now we need an optimizer to define our weight update rule. The PyTorch `optim` package provides a variety of the most common optimizers, such as SGD, RMSProp, and Adam. For this example we will use Adam.\n",
    "\n",
    "We will need to provide the optimizer with all of the parameters that we want it to update, so in this case we will pass it all of the CNN parameters. \n",
    "\n",
    "The ability to control which parameters are updated can be extremely useful. For example, you can configure the optimizer to update different parts of your model with differet learning rates. Another use case would be for fine-tuning pretrained model, where you only need to update the parameters in the last layer. Multiple optimizers could also be used, such as in the case of training model with multiple parts such as GANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Augmentation\n",
    "Before we can train the network we need to have some data for it. The `torchvision` package provides data loaders for several of the most common datasets (MNIST, CIFAR, MS-COCO, etc.) as well as a generic data loader that can load images from a pre-defined folder structure.\n",
    "\n",
    "Dataset preprocessing and augmentation can be added to the data loaders through the use of the `torchvision.transforms` package. Options include normalization, scaling, cropping, and flipping. The external `torchsample` library extends these options further with additional augmentation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a list of transformations to be applied to each image\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = dsets.MNIST(root='data/',\n",
    "                            train=True, \n",
    "                            transform=transforms,\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='data/',\n",
    "                           train=False, \n",
    "                           transform=transforms)\n",
    "\n",
    "# Data loaders define how the dataset is sampled\n",
    "# They can be configured to use multiple threads if necessary\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=128, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=128, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the images we've got. The `torchvision.utils.makegrid` function is great for this. Pass it a 4D tensor (B, C, H, W) and it returns a nice grid of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8b78c84c50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJkAAAD8CAYAAABkZQZTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuJLFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b78bb64d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_gen = iter(test_loader)\n",
    "test_images, test_labels = test_gen.next()\n",
    "\n",
    "image_grid = make_grid(test_images, normalize=True).numpy()  # We need to convert from torch tensor to numpy\n",
    "image_grid = np.moveaxis(image_grid, 0, 2)                   # The chanel dimension is first, but we need it last\n",
    "plt.imshow(image_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Now that we have all of the parts we can put them together in the training loop. For each batch in the training loop we will retrieve a batch of images and labels from the data loader and wrap them as Variables.\n",
    "\n",
    "Next we will pass the images through the network and calculate the loss using our previously defined criterion. To calculate gradients via backprop we apply the `.backward()` function on the output of the loss function. \n",
    "\n",
    "Finally we will take a `.step()` with the optimizer to update the model parameters.\n",
    "\n",
    "Note that before performing a pass through the network we should call `.zero_grad`. You can call this either on the optimizer or network. In either case the gradients accumulated on the associated parameters are reset to 0. The reason we do this is because PyTorch automatically accumulates gradients each time `.backward()` is called, so they must be manually cleared after each weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-095ef00c1e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# Perform forward pass through network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate the categorical cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# Backpropogate errors through network to get gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/terrance/pkgs/anaconda2/envs/torch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f402cb41ac93>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/terrance/pkgs/anaconda2/envs/torch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/terrance/pkgs/anaconda2/envs/torch/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/terrance/pkgs/anaconda2/envs/torch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/terrance/pkgs/anaconda2/envs/torch/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 237\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/terrance/pkgs/anaconda2/envs/torch/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     38\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     39\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    # Wrap the data loader in a tqdm progress bar so it looks fancy\n",
    "    progress_bar = tqdm_notebook(train_loader, desc='Epoch ' + str(epoch))\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        cnn.zero_grad()\n",
    "        pred = cnn(images)              # Perform forward pass through network\n",
    "        loss = criterion(pred, labels)  # Calculate the categorical cross entropy loss\n",
    "        loss.backward()                 # Backpropogate errors through network to get gradients\n",
    "        optimizer.step()                # Update weights using the previously defined update rule\n",
    "        \n",
    "        progress_bar.set_postfix(\n",
    "            xentropy_loss='%.3f' % loss.data[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That loop may have taken a while since it was running on the CPU. Let's speed it up by transferring our model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cnn.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function\n",
    "\n",
    "Let's also define a function so that we can evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 74.09%\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    cnn.eval()    # Change model to 'eval' (only necessary if using dropout or batchnorm)\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images).cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = cnn(images)\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    cnn.train()\n",
    "    return 100 * correct / total\n",
    "    \n",
    "print('Test accuracy: ' + str(test_model()) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Training\n",
    "\n",
    "If we train the model now on the GPU it should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.77%\n"
     ]
    }
   ],
   "source": [
    "# Because we already ran the model on the CPU we need to reinitialize the optimizer for the GPU\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    # Wrap the data loader in a tqdm progress bar so it looks fancy\n",
    "    progress_bar = tqdm_notebook(train_loader, desc='Epoch ' + str(epoch))\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        images = Variable(images).cuda()  # Now we need to make sure out tensors are on the GPU as well\n",
    "        labels = Variable(labels).cuda()\n",
    "        \n",
    "        cnn.zero_grad()\n",
    "        pred = cnn(images)              # Perform forward pass through network\n",
    "        loss = criterion(pred, labels)  # Calculate the categorical cross entropy loss\n",
    "        loss.backward()                 # Backpropogate errors through network to get gradients\n",
    "        optimizer.step()                # Update weights using the previously defined update rule\n",
    "        \n",
    "        progress_bar.set_postfix(\n",
    "            xentropy_loss='%.3f' % loss.data[0])\n",
    "        \n",
    "    print('Test accuracy: ' + str(test_model()) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training\n",
    "\n",
    "We can also expend to using more than a single GPU with the PyTorch `DataParallel` container. As long as the batch size is evenly divisible by the number of available GPUs, PyTorch will automatically distribute computations among the hardware.\n",
    "\n",
    "We can do this in a single line, by putting out model in a `DataParrallel` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn = torch.nn.DataParallel(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half Precision\n",
    "\n",
    "What if we have a model that takes up more memory than we have available on the GPU? We can cut the memory requirements in half by using half precision to store the model weights. Note that during the forward pass full 32 bit precision is still used, so we won't save anything on computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn = cnn.half()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
